{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Installs ###\n",
    "# %pip install langchain openai pypdf faiss-cpu python-dotenv tiktoken"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Documentation\n",
    "\n",
    "__NEED AN OPENAI API KEY TO RUN__\n",
    "\n",
    "## Environment Setup\n",
    "- Load environment variables using `dotenv`.\n",
    "- Import necessary libraries like `os`, `glob`, and various modules from `langchain`.\n",
    "\n",
    "## Finding PDF Files\n",
    "- Function `find_pdf_files(directory)` lists all PDF files in a specified directory and subdirectories.\n",
    "- Uses `glob.glob` for searching and sorts the absolute paths of the PDF files.\n",
    "\n",
    "## Loading and Splitting Text\n",
    "- `load_and_split(path)` loads a PDF and splits its text into chunks.\n",
    "- Utilizes `PyPDFLoader` for reading PDF content.\n",
    "- `CharacterTextSplitter` breaks text into 1000 character chunks, separated by newline (`\\n`).\n",
    "\n",
    "## Setup PDF Files and Embeddings\n",
    "- Defines a list of file paths to various PDF documents.\n",
    "- Creates an embedding model using `OpenAIEmbeddings`.\n",
    "\n",
    "## Creating and Merging Vector Stores\n",
    "- For each PDF, the text is loaded, split, and converted into vector representations using `FAISS`.\n",
    "- Merges these vector stores into one primary store.\n",
    "\n",
    "## Saving and Loading Vector Store\n",
    "- Saves the merged vector store locally.\n",
    "- Reloads it using `FAISS.load_local`.\n",
    "\n",
    "## Demonstration with QA System\n",
    "- Sets up a QA system using `RetrievalQA` with an `OpenAI` language model and the loaded vector store.\n",
    "- Runs example queries to demonstrate information retrieval from the processed PDF documents.\n",
    "\n",
    "This setup establishes a document retrieval system using embeddings and a QA model, capable of answering questions based on content from the loaded PDF documents.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "import os\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.llms import OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_pdf_files(directory):\n",
    "    os.chdir(directory)\n",
    "\n",
    "    pdf_files = []\n",
    "\n",
    "    for file in glob.glob('**/*.pdf', recursive=True):\n",
    "        absolute_path = os.path.abspath(file)\n",
    "        pdf_files.append(absolute_path)\n",
    "        \n",
    "    pdf_files.sort()\n",
    "\n",
    "    return pdf_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_split(path):\n",
    "    loader = PyPDFLoader(file_path=path)\n",
    "    documents = loader.load()\n",
    "\n",
    "    # A chunk size of 1000 characters offers a balance between granularity and manageability\n",
    "    # It's large enough to contain meaningful units of text (like sentences or paragraphs),\n",
    "    # but small enough to be easily processed by various algorithms\n",
    "    # The absence of overlap means each character in the document is only processed once\n",
    "    text_splitter = CharacterTextSplitter(\n",
    "            chunk_size=1000, chunk_overlap=0, separator=\"\\n\"\n",
    "        )\n",
    "    return text_splitter.split_documents(documents=documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_files = [\n",
    "    \"/Users/aditkapoor/Local Documents/Work/Cognizant/langchain-proj/assets/data/gray-city.pdf\",\n",
    "    \"/Users/aditkapoor/Local Documents/Work/Cognizant/langchain-proj/assets/data/irl.pdf\",\n",
    "    \"/Users/aditkapoor/Local Documents/Work/Cognizant/langchain-proj/assets/data/singing-peddler.pdf\",\n",
    "    \"/Users/aditkapoor/Local Documents/Work/Cognizant/langchain-proj/assets/data/memoirs.pdf\",\n",
    "    \"/Users/aditkapoor/Local Documents/Work/Cognizant/langchain-proj/assets/data/small-little-circle.pdf\",\n",
    "    \"/Users/aditkapoor/Local Documents/Work/Cognizant/langchain-proj/assets/data/veracious.pdf\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings()\n",
    "vectorstores = []\n",
    "\n",
    "for file_path in pdf_files:\n",
    "    docs = load_and_split(file_path)\n",
    "    \n",
    "    # I chose to use the FAISS vectorstore because it's the fastest to work with locally as I often\n",
    "    # needed to rebuild the vectorstore when I made changes to the code. With Pinecone, it was harder\n",
    "    # to make changes as I had to reset the vectorstore on the website instead of just deleting the locally\n",
    "    # saved stone (as FAISS manages it)\n",
    "    vectorstore = FAISS.from_documents(docs, embeddings)\n",
    "    vectorstores.append(vectorstore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEBUG CELL ###\n",
    "# vectorstores[0].docstore._dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, len(vectorstores)):\n",
    "    vectorstores[0].merge_from(vectorstores[i])\n",
    "    \n",
    "vectorstores[0].save_local(\"faiss_index_project\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings()\n",
    "new_vectorstore = FAISS.load_local(\"faiss_index_project\", embeddings)\n",
    "qa = RetrievalQA.from_chain_type(llm=OpenAI(), chain_type='stuff', retriever=new_vectorstore.as_retriever())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Clyde yelled at his mom.\n"
     ]
    }
   ],
   "source": [
    "result = qa.run(\"Who did Clyde yet at about giving third person narrators too much information? If you don't know, say you don't know.\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The father was sad after receiving the letter because it was from his son, who had been shot two weeks earlier. He was writing to tell his parents he was coming home, but he wouldn't make it back before the letter arrived.\n"
     ]
    }
   ],
   "source": [
    "result = qa.run(\"In the memoir, why was the father sad after receiving the letter?\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The patches on people's clothing in the crowded streets of Delhi tell stories of the hardships and struggles of life in India. They symbolize the diversity of the city and represent different times in history, from the air-conditioned car safe from the scalding ground to the hand-me-downs that are too small. They also emphasize the contrast between the huge mansions and the slums, highlighting the disparity between the wealthy and the poor.\n"
     ]
    }
   ],
   "source": [
    "result = qa.run(\"In the crowded streets of Delhi, what stories do the patches on people's clothing tell, as described in the poem? Reflect on the diversity and history embedded in these patches.\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "IRL portrays the world post-internet and modern relationships by showing how people can build relationships and connections through the internet and then continue those relationships in real life. The story follows Sahil and Magnus who build a friendship online and eventually meet IRL (in real life) to continue their connection.\n"
     ]
    }
   ],
   "source": [
    "result = qa.run(\"How does \\\"IRL\\\" portray the world post-internet and modern relationships?\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " In \"Veracious\", the author is mad at his Uncle despite his untimely death because his death has caused so much pain for his family. His death has left his grandmother alone in a large house and has reduced his mother to a vulnerable state. In addition, his death came only two years after the passing of his older brother, making the pain even greater. Furthermore, his mother had recently told the author that Uncle Scott was doing better, sobering up and planning to work for Uber. The author resents his Uncle for leaving his family in such a difficult and painful situation, feeling that he was being selfish in death.\n"
     ]
    }
   ],
   "source": [
    "result = qa.run(\"In \\\"Veracious\\\", why is the author so mad at his Uncle despite his untimely death? Give me around 5 sentences.\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The author was very attached to Maggi because she provided him comfort during a difficult time. He was grieving the death of one of his students and felt responsible for her death. Maggi was understanding and comforting, and the author felt like he could talk to her in a way he couldn't talk to anyone else.\n"
     ]
    }
   ],
   "source": [
    "result = qa.run(\"Describe the Author's relationship with Maggi. Why was he so attached to her?\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"  # Fill in with your own prompt\n",
    "result = qa.run(prompt)\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
