{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Installs ###\n",
    "# %pip install langchain openai pypdf faiss-cpu python-dotenv tiktoken"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Documentation\n",
    "\n",
    "__⚠️ NEED AN OPENAI API KEY TO RUN ⚠️__\n",
    "\n",
    "## Environment Setup\n",
    "- Load environment variables using `dotenv`.\n",
    "- Import necessary libraries like `os`, `glob`, and various modules from `langchain`.\n",
    "\n",
    "## Finding PDF Files\n",
    "- Function `find_pdf_files(directory)` lists all PDF files in a specified directory and subdirectories.\n",
    "- Uses `glob.glob` for searching and sorts the absolute paths of the PDF files.\n",
    "\n",
    "## Loading and Splitting Text\n",
    "- `load_and_split(path)` loads a PDF and splits its text into chunks.\n",
    "- Utilizes `PyPDFLoader` for reading PDF content.\n",
    "- `CharacterTextSplitter` breaks text into 1000 character chunks, separated by newline (`\\n`).\n",
    "\n",
    "## Setup PDF Files and Embeddings\n",
    "- Defines a list of file paths to various PDF documents.\n",
    "- Creates an embedding model using `OpenAIEmbeddings`.\n",
    "\n",
    "## Creating and Merging Vector Stores\n",
    "- For each PDF, the text is loaded, split, and converted into vector representations using `FAISS`.\n",
    "- Merges these vector stores into one primary store.\n",
    "\n",
    "## Saving and Loading Vector Store\n",
    "- Saves the merged vector store locally.\n",
    "- Reloads it using `FAISS.load_local`.\n",
    "\n",
    "## Demonstration with QA System\n",
    "- Sets up a QA system using `RetrievalQA` with an `OpenAI` language model and the loaded vector store.\n",
    "- Runs example queries to demonstrate information retrieval from the processed PDF documents.\n",
    "\n",
    "This setup establishes a document retrieval system using embeddings and a QA model, capable of answering questions based on content from the loaded PDF documents.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "import os\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.llms import OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_pdf_files(directory):\n",
    "    os.chdir(directory)\n",
    "\n",
    "    pdf_files = []\n",
    "\n",
    "    for file in glob.glob('**/*.pdf', recursive=True):\n",
    "        absolute_path = os.path.abspath(file)\n",
    "        pdf_files.append(absolute_path)\n",
    "        \n",
    "    pdf_files.sort()\n",
    "\n",
    "    return pdf_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_split(path):\n",
    "    loader = PyPDFLoader(file_path=path)\n",
    "    documents = loader.load()\n",
    "\n",
    "    # A chunk size of 1000 characters offers a balance between granularity and manageability\n",
    "    # It's large enough to contain meaningful units of text (like sentences or paragraphs),\n",
    "    # but small enough to be easily processed by various algorithms\n",
    "    # The absence of overlap means each character in the document is only processed once\n",
    "    text_splitter = CharacterTextSplitter(\n",
    "            chunk_size=1000, chunk_overlap=0, separator=\"\\n\"\n",
    "        )\n",
    "    return text_splitter.split_documents(documents=documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = \"/Users/aditkapoor/Local Documents/Work/Cognizant/langchain-proj/assets/data\"\n",
    "pdf_files = [\n",
    "    f\"{base_path}/gray-city.pdf\",\n",
    "    f\"{base_path}/irl.pdf\",\n",
    "    f\"{base_path}/singing-peddler.pdf\",\n",
    "    f\"{base_path}/memoirs.pdf\",\n",
    "    f\"{base_path}/small-little-circle.pdf\",\n",
    "    f\"{base_path}/veracious.pdf\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings()\n",
    "vectorstores = []\n",
    "\n",
    "for file_path in pdf_files:\n",
    "    docs = load_and_split(file_path)\n",
    "    \n",
    "    # I chose to use the FAISS vectorstore because it's the fastest to work with locally as I often\n",
    "    # needed to rebuild the vectorstore when I made changes to the code. With Pinecone, it was harder\n",
    "    # to make changes as I had to reset the vectorstore on the website instead of just deleting the locally\n",
    "    # saved stone (as FAISS manages it)\n",
    "    vectorstore = FAISS.from_documents(docs, embeddings)\n",
    "    vectorstores.append(vectorstore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEBUG CELL ###\n",
    "vectorstores[0].docstore._dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, len(vectorstores)):\n",
    "    vectorstores[0].merge_from(vectorstores[i])\n",
    "    \n",
    "vectorstores[0].save_local(\"faiss_index_project\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings()\n",
    "new_vectorstore = FAISS.load_local(\"faiss_index_project\", embeddings)\n",
    "qa = RetrievalQA.from_chain_type(llm=OpenAI(), chain_type='stuff', retriever=new_vectorstore.as_retriever())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Clyde's mom.\n"
     ]
    }
   ],
   "source": [
    "result = qa.run(\"Who did Clyde yet at about giving third person narrators too much information? If you don't know, say you don't know.\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The father was sad after receiving the letter because it was the last letter from his son, who had been shot two weeks earlier while writing the letter.\n"
     ]
    }
   ],
   "source": [
    "result = qa.run(\"In the memoir, why was the father sad after receiving the letter?\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The patches on people's clothing tell stories of poverty and inequality, of harsh and unfair circumstances, of people living in cramped and overcrowded slums while other people live in huge mansions with luxurious amenities. The patches also reflect the history and diverse culture of the city, with its mix of poverty and wealth, its many languages and religions.\n"
     ]
    }
   ],
   "source": [
    "result = qa.run(\"In the crowded streets of Delhi, what stories do the patches on people's clothing tell, as described in the poem? Reflect on the diversity and history embedded in these patches.\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "IRL showcases how the internet has changed the way people interact and build relationships. It highlights how people can become close friends even if they have never met in person, and how these relationships can be just as meaningful as any other kind of friendship. It also shows how technology has enabled people to quickly and easily connect with one another, no matter where they are in the world.\n"
     ]
    }
   ],
   "source": [
    "result = qa.run(\"How does \\\"IRL\\\" portray the world post-internet and modern relationships?\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " In \"Veracious\", the author is so mad at his Uncle despite his untimely death for a number of reasons. Firstly, the author felt like his Uncle was selfish for leaving his grandmother, JoJo, alone in the house with no one to take care of her. Secondly, the author felt that his Uncle's death had reduced his mother to a vulnerable and powerless state. Thirdly, his Uncle's death came just two years after the passing of his older brother, making his mother have to deal with two losses in a short period of time. Fourthly, the author was angry that his Uncle had been doing better and was planning to get a job and move out of JoJo's place. Finally, the author felt betrayed because he had looked up to his Uncle as a child and had felt that his Uncle was taking steps to make his life better.\n"
     ]
    }
   ],
   "source": [
    "result = qa.run(\"In \\\"Veracious\\\", why is the author so mad at his Uncle despite his untimely death? Give me around 5 sentences.\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The author was deeply attached to Maggi, as she was a source of comfort for him in a difficult time. He felt that she was understanding and intelligent, and he was able to tell her his story without judgement. He found her presence calming and her rhythmic breathing helped him to sleep.\n"
     ]
    }
   ],
   "source": [
    "result = qa.run(\"Describe the Author's relationship with Maggi. Why was he so attached to her?\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " I don't know.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Who is the current president of the United States\"  # Fill in with your own prompt\n",
    "result = qa.run(prompt)\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
